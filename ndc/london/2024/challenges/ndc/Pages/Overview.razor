@page "/overview"
<PageTitle>OpenAI</PageTitle>

<h3>NDC London Workshop </h3>

<dl>
    <dt>Generative AI (GenAI) Overview</dt>
    <dd>
        GenAI use learning patterns from data they’re trained on to create new things like music, images, text, and other content,
        like poetry, or blog posts.

    </dd>
    <dd>
        <img src="images/genai.png" width="600px" />
    </dd>
    <dd>
        <img src="images/comp.png" width="600px" />
    </dd>
</dl>
<dl>
    <dt>Tools</dt>
    <dd>
        We'll use several tools in this workshop.
    </dd>
    <dd>
        <ul>
            <li>OpenAI LLMs (GPT 3.5 / GPT 4.0)</li>
            <li>OpenAI API</li>
            <li>Azure OpenAI Library for .NET (wrapper for the OpenAI API)</li>
            <li>Visual Studio - Community Edition w/ Blazor plugins</li>
            <li>Blazor</li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>OpenAI</dt>
    <dd>
        Who is OpenAI? They are the creators of ChatGPT.
    </dd>
    <dd>
        But they have more than just ChatGPT: Whisper, DALL-E, and Codex.
    </dd>
    <dd>
        <img src="images/wopenai.jpeg" width="600px" />
    </dd>
</dl>
<dl>
    <dt>Real World Use Cases</dt>
    <dd>
        There are several real world use cases for ChatGPT.
    </dd>
    <dd>
        <ul>
            <li>Virtual Assistants / Intelligent Applications</li>
            <li>Dynamic Content Personalization</li>
            <li>Code Assistance</li>
            <li>Automated Summarization</li>
            <li>Interactive Storytelling</li>
            <li>Language Translation</li>
            <li>Data Interpretation</li>
            <li>Image Generation</li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>Demo of ChatGPT</dt>
    <dd>
        Let interact with ChatGPT from the UI.
    </dd>
</dl>
<dl>
    <dt>Key Concepts</dt>
    <dd>
        Here are the common terms when working with LLMs.
    </dd>
    <dd>
        <ul>
            <li>
                <b>Model - </b>
                Large language models (LLM) are very large deep learning models (i.e. software algorithm) that are pre-trained on vast amounts of data.
                The underlying architecture, transformer, is a set of neural networks that consist of an encoder and a decoder
                with self-attention capabilities. ChatGPT for example, is a Large Language Model, or LLM.  When we request information from the OpenAI API,
                we will specify which model we want to use.

                &nbsp;
                Check out my keynote on Friday: <b>Transformers: the Rise of ChatGPT</b>.
            </li>
            <li><b>Prompt - </b>Guides the model on the desired output. The more informatiom (e.g., instruction, examples, etc.) you provide, the better the model performs. </li>
            <li><b>Completion - </b>Represents the responses generated by the LLM based on the given prompt. The model completes the prompt by predicting and producing a sequence of text.</li>
            <li>
                <b>Token - </b>In natural language processing, text is often broken down into smaller units called tokens. Tokens are the fundamental units of text that LLMs
                operate on. These can be individual words, subwords, or characters, depending on the tokenization scheme.

                Token-based pricing is a common model used by LLM platforms. Users are charged based on the number of tokens processed, including both
                input tokens (in the prompt) and output tokens (in the completion). Therefore, longer prompts and more extensive completions generally
                translate to higher costs.
            </li>
            <li>
                <b>Completion Parameters -</b> There are parameters that can be used to affect how consistent or random the completions
                will be. Such as <b>“temperature”</b> and <b>“top_p”</b>. <b>Temperature</b> value ranges from 0 to 1 and helps the model determine which word is selected next.
                Lower values like 0 mean the model will always return “ best” next because it has the highest probability. As you increase the temperature,
                moving it closer to 1, the model will take more risks and consider tokens with lower probabilities, like “with” or “better.” Higher
                temperatures may be helpful to increase the randomness of the model, providing more variety and creativity. <b>Top_p</b> Tells the model to return the top
                responses that meet your desired probability score. For example, a value of 0.1 means only words in the top 10% probability score are returned.

                Next we have the top_p parameter which stands for “top probability” and it’s an alternative to temperature. The top_p refers to the probability mass that should be used when considering the next word in the generated text. Essentially it sets a threshold for the probability of the next word being chosen and only considers the most likely words that exceed that threshold. This means that with higher top_p values, the language model will be more conservative with its predictions.
                In general, you should use top_p to control the coherence of the generated text, but if you want to affect creativity and predictabiltiy of the text, then you should use temperature. OpenAI recommends using either temperature or top_p, but not both.

            </li>
            <li>
                <b>Fine-tuning - </b>Once you have a foundation LLM, you can fine-tune it using data for your specific use case. During fine-tuning, you start with the
                pre-trained GPT model and train it further on a smaller dataset, which makes running the model cheaper and faster.
            </li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>Pricing</dt>
    <dd>
        OpenAI offers a <b>pay-as-you-go </b> pricing model, where you’re billed at the end of each calendar month. 

        There are various models at different price points.
        <ul>
            <li><b>GPT-3.5 Turbo - </b><img src="images/gpt35turbo.png" width="300px" /></li>
            <li><b>GPT-4 -</b> <img src="images/gpt4.png" width="300px" /></li>
            <li><b>GPT-4 Turbo - </b><img src="images/gpt4turbo.png" width="300px" /></li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>Prompt Engineering Overview</dt>
    <dd>
        Prompt engineering focuses on the thoughtful design of prompts to elicit correct answers from Large Language Models
        (or LLMs) like ChatGPT.Prompt engineering is crucial for achieving optimal results with language models. Utilizing advanced
        prompting techniques can significantly enhance the performance of language models by guiding them to generate more accurate,
        coherent, and relevant outputs. Overall, prompt engineering empowers us to harness the capabilities of language models effectively,
        ensuring they align with specific use cases, minimize biases, and deliver high-quality outputs.
    </dd>
    <dd>
        <ul>
            <li><b>Zero-Shot - </b>Zero-shot refers to a model making predictions without additional training within the prompt.</li>
            <li><b>Few-Shot - </b>Few-shot is when LLM is given a few examples in the prompt for it to more quickly adapt to new examples.</li>
            <li>
                <b>Chain of Thought (CoT) - </b>CoT prompting is an advanced prompting technique that generates a chain of thought or a series
                of intermediate reasoning steps that improve the model’s ability to perform complex reasoning.

                This prompting technique enables models to decompose multi-step problems.
            </li>
            <li><b>Retrieval-augmented generation (RAG)- </b>Retrieval Augmented Generation (RAG) enhances LLMs by integrating real-time,
            external knowledge, improving the quality of their responses.
            </li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>ChatGPT Hands-on</dt>
    <dd>
        <ul>
            <li>Sign up for ChatGPT (<a href="https://chat.openai.com/">https://chat.openai.com/</a>) and play around with few-shot prompting. </li>
            <li>Share with the group your prompts and responses. </li>
        </ul>
    </dd>
</dl>
<dl>
    <dt>OpenAI API</dt>
    <dd>
        You can access ChatGPT programmatically via the OpenAI API.
    </dd>
    <dd>
        Let's review the API:
        <ul><li><a href="https://platform.openai.com/docs/introduction">Documentation</a></li>
            <li><a href="https://platform.openai.com/docs/api-reference">API reference</a></li></ul>
    </dd>
    <dd>
        Let's test the API from Postman (or cURL).
    </dd>
</dl>